# -*- coding: utf-8 -*-
"""Copy of Hull_HW13_ML_Mod.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pTFLFd4W2P5ARacIv07drXrG-9In1Rd

----------------------
# Change Log
* QH 12012020
    * Added Google Collab Google Sheet Functionality
      * https://colab.research.google.com/notebooks/snippets/sheets.ipynb#scrollTo=JiJVCmu3dhFa
    * Added some stuff from ML Methods other resources
      * https://github.com/jfzhang95/LSTM-water-table-depth-prediction/
    * Also:
      * Different 'Windows'
      * MultiVariable Regression
      * R^2
      * Drop_Out
      * New NetWork Architecture
      * Hardware Acceleration
     
* QH 11252020
    * Added to google collab Hull_HW13.ipynb from HASTools Submissions Repo
    * Simplified to focus on streamflow
    * Added elements of PyTorch to facilitate LSTM-style prediction from https://colab.research.google.com/drive/1XmOoVu8Tt4iSCTrGXYBpbyD8Tq8RSvoh?authuser=1#scrollTo=vIWvJCpOVmwU

* QH 11222020
    * Modified from Hull_HW12.py in submissions directory

* QH 11132020
    * Modified from Week12_Reanalysis_Netcdf_starter.py in course materials

* QH 11142020
    * Modified from Hull_HW_12_Week12_Forecast_Netcdf_starter.py in Week12 HW dir
    * Modified from Hull_HW10.py in submissions dir
    * Modified form Hull_HW12_MAP.py in Week12 HW dir

----------------------

# Introduction

The function is to assemble hydrologic and meterological data to create streamflow forecasts for streamflow at Verde River in central Arizona. See the outline below for a detailed description of the code utilized by this notebook. In general we are comparing traditional ML (ie linear autocorrelation) techniques with 'novel' DL (i.e. LSTM) techniques

# Outline of 'how' I generate the forecast

## Data Assembly and Prep
1. Define modules
1. Define functions
1. Define globals
1. Import forecast 'T' and 'P' - Virtual Crossing
1. Import and assemble historical flow - USGS
1. Import and assemble historical 'T' and 'P' - Mesonet
1. Resample Datasets
1. Cross Plots Before Normalization
1. Subset, normalize, and lag datasets
1. Cross Plots After Normalization

## Regression
1. Build and Export an Auto-Regressive Model
1. Show fit between real and training data

## PyTorch LSTM (2)
1. Functions
1. Globals
1. Code

## PyTorch LSTM (1) 
1. Functions
1. Globals
1. Import Data
1. Train Model
1. Score the LSTM and Compare to Reality

## Compare Predicted Values From Different Methods

# Data Assembly and Prep
* Assembling all data

## 1. Define Modules
* Note that forecast-specific functions are located in the script `Hull_HW13_fxns.py` within this directory
"""

# generic
import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
from pandas.plotting import scatter_matrix

# functions
import json
import urllib.request as req
import urllib
from sklearn.linear_model import LinearRegression

# for scaling
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer

# for scoring
from sklearn.metrics import r2_score, mean_squared_error

# for DL
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

# google sheets
from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials

"""## 2. Define Functions
* Including functions that used to be stored in a different directory (from Hull_HW13_fxns.py 11252020)
"""

# ----------------------------------------------------------------------------------
# Define functions
# ----------------------------------------------------------------------------------
def clean_dataset(df):
    """Removes all infinity, nan, and numbers out of range
    from: https://stackoverflow.com/questions/31323499/
    sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for
    """
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)


def makemodel(x, y):
    """returns a multiple regression model

       using sklearn linear regression tool

       returns the model object = model
       returns the score = score

       takes 2 required variables, x and y
       x is the predictive (independent) variable(s)
       y is the predicted (dependent) variable

       both x and y need to be pandas dataframes, where x
       contains ALL of the predictive (independent) variables
       to be used.

       IMPORTANT: 'x' needs to be a list of column titles, even
       if only one predictive variable is passed

       if dimensions[x] = 1, then single predictive variable
       if dimensions[x] > 1, then multiple predictive variables

       example:
       x = train_week[['flow_tm1']] # use double brackets here
       y = train_week['flow'] # use single brackets here
       m, x = makemodel(x,y)
       """

    model = LinearRegression()
    y = y.values
    if x.shape[1] == 1:
        x = x.values.reshape(-1, 1)
    model.fit(x, y)
    score = model.score(x, y)
    return model, score


def extractmasonet(base_url, args):
    """takes
       1) a string of 'base_url'
       2) a dictionary of api arguments
       3) a string of 'token'
       specific to the metamot API

       See more about metamot:
       https://developers.synopticdata.com/mesonet/explorer/
       https://developers.synopticdata.com/about/station-variables/
       https://developers.synopticdata.com/mesonet/v2/getting-started/

       returns a dictionary
       containing the response of a JSON 'query'
       """

    # concat api arguments (careful with commas)
    apiString = urllib.parse.urlencode(args)
    apiString = apiString.replace('%2C', ',')

    # concat the API string to the base_url to create full_URL
    fullUrl = base_url + '?' + apiString
    print('full url =', fullUrl, '\n')

    # process data (use url to query data)
    # return as dictionary
    response = req.urlopen(fullUrl)
    responseDict = json.loads(response.read())

    return responseDict


def assemble_data_masonet(base_url, args, stationDict,
                          data_join,
                          station_condition='ACTIVE',
                          station_name=False):
    """takes the basics for a data extraction
        and pulls out only the stations that meet a certain condition

        base_url = url at masonet for extraction
        args = arguments, including token and parameters
        station_Dict = a previously created response Dictionary
            of stations
        data_join = an external pandas dataset to join the data to.
            Must contain a datetime index
        station_condition = the condition used to crete response for data
            default is to look for only active stations. Only acceptable
            values are 'ACTIVE' and 'INACTIVE'
        station_name = by default FALSE. If specified a string, will
            only look for stations by the name specified.

        note by default resamples the data daily on the max

        returns a panda dataframe containint he data wanted
        """

    # 2b) Assemble all relevant dictionaries into a list, based on station name
    stationList = []
    for station in stationDict['STATION']:
        # station name and if is active
        print(station['STID'], station['STATUS'], station["PERIOD_OF_RECORD"],
              "\n")
        # time series data args
        args['stids'] = station['STID']
        # extract data from active/inactive stations
        if station['STATUS'] == station_condition:
            # if a station name is specified
            if station_name is not False:
                if (station['STID'] == station_name):
                    # extract data
                    responseDict = extractmasonet(base_url, args)
                    # create a list of all stations
                    stationList.append(responseDict)
            # if station name is not specified
            else:
                # extract data
                responseDict = extractmasonet(base_url, args)
                # create a list of all stations
                stationList.append(responseDict)

    # Checks to see if the API Call returned valid data
    if stationList[0]['SUMMARY']['RESPONSE_CODE'] == -1:
        print(stationList[0]['SUMMARY']['RESPONSE_MESSAGE'])
        return "nothing"

    # 2d) convert all data pd
    # list of keys under observations (for use in inner loop)
    for station in stationList:
        # if station id has the station_name, or station_name is False
        if (station['STATION'][0]['STID'] == station_name) or \
                            (station_name is False):
            print(station['STATION'][0]['STID'])
            for key, value in station["STATION"][0]['OBSERVATIONS'].items():
                # creates a list of value related to key
                # temp = station["STATION"][0]['OBSERVATIONS'][key]
                if (key == 'date_time'):
                    # create index
                    df = pd.DataFrame({key: pd.to_datetime(value)})
                else:
                    # concat df
                    df = pd.concat([df, pd.DataFrame({key: value})], axis=1)
            # # set index for df
            df = df.set_index('date_time')
            # resample on day
            df = df.resample('D').max()
            # join df to data dataframe
            data_join = data_join.join(df,
                                       rsuffix="_"+station['STATION'][0]['STID'
                                                                         ])
            df = pd.DataFrame()
    return data_join


def Kelvin_2_Faren(K_temp):
    """takes a temperature in Kelvin

    returns a temperature in Fareignheit
        """
    return (K_temp - 273.15)*(9/5) + 32


def norm_it(startdate, enddate, dfin, dfname, l_back=1, toscale=True,
            cscaler=MinMaxScaler(feature_range=(0, 1))):
    """
    This function noramlizes a column of data from a pandas dataframe
    using the predefined scaler feature of sklearn
    to create a ~ normal distribution
    and allow for better fitting between different types of variables
    in a multivariable regression

    It also lags the data by a specified number of weeks (look_back)

    Takes:
    A start and end date (strings)
        startdate =
        enddate =
    A dataframe (pandas)
        dfin =
    The name of a single column of data to normalize (string)
        dfname =
    A specified number of look backs (integer)
        l_back = [1]
    A Boolean to decide if to scale the data (i.e. if not desired or already done)
        toscale = [True]
    An optional scaler
        cscaler = [MinMaxScaler(feature_range=(0, 1))]

    Returns:
    The dataframe with a column of normalized, and lagged, data
    The scaler model that can be used to 'inverse' transform
        """

    # # subset
    dfin = dfin.loc[startdate:enddate]
    if toscale is True:
        # # normalize
        scaler = cscaler
        # # add normalized to dataset
        dfin[dfname+'_norm'] = scaler.fit_transform(dfin[
                                                dfname].to_numpy().reshape(-1, 1))
        # # lag
        dfin[dfname+'_norm'+str(l_back)] = dfin[dfname+'_norm'].shift(l_back)
        return dfin, scaler
    else:
        dfin[dfname+str(l_back)] = dfin[dfname].shift(l_back)
        return dfin



def denorm_it(val, scaler):
    """De normalizes a single value

    Takes:
    A scaled value (a single number)
        val =
    A scaler from sklearn
        scaler =
    """
    # # inverse transform a single value
    newval = scaler.inverse_transform(val.reshape(1, -1))
    return newval

"""## 3. Define Globals
* Including the max and min dates used to extract data via API from data source, as well as the date ranges used to run regression
"""

# General
start = '2010-02-01'
end = '2020-12-31'
window = 4 # used to established sliding window in first LSTM method

"""## 4. Import forecast 'T' and 'P' - Virtual Crossing
* Note that a limited number of 'free' API queries are allowed per day (~1000)
* It may be necessary for user to change 'vskey' by creating a new virtual crossing username and locating account key at this website: https://www.visualcrossing.com/weather/weather-data-services#/accountdetails
"""

# keys for accessing forecast
vskey = 'SL1F2XI46LCJAUMYAGG6GMV5B'
vslat = str(34.448333)
vslong = str(-111.789167)
vsurl = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?locations='+vslat+','+vslong+'&aggregateHours=24&unitGroup=us&shortColumnNames=false&contentType=csv&key='+vskey

# access forecast
vc_forecast_df = pd.read_csv(vsurl,
                             parse_dates=['Date time'],
                             index_col='Date time'
                             )

"""## 5. Import and assemble historical flow - USGS"""

# adjust path as necessary
site = '09506000'
url = "https://waterdata.usgs.gov/nwis/dv?cb_00060=on" \
      "&format=rdb&site_no="+site+"&referred_module=sw&" \
      "period=&begin_date="+start+"&end_date="+end

# read in data
flow_df = pd.read_table(url, sep='\t', skiprows=30,
                        names=['agency_cd', 'site_no',
                               'datetime', 'flow', 'code'],
                        parse_dates=['datetime'],
                        index_col='datetime'
                        )

# re-instantiate data with just the flow values
flow_df = np.log(flow_df[['flow']])
# flow_df = flow_df[['flow']]
flow_df.index = flow_df.index.tz_localize(tz="UTC")

"""## 6. Import and assemble historical 'T' and 'P' - Mesonet"""

# 1) quickly look for nearby stations
# 1a) Token
# # IMPORTNAT: I overused my token ('a836998da79e4faeac2bf7f5cda57a6e')
# # so I am only able to use the demo token below
mytoken = 'demotoken'
# 1b) 'Base' URL
base_url = "https://api.synopticdata.com/v2/stations/metadata"
# 1c) nearby stations
# look for 10 nearest stations within 10 miles of usgs gaging station
args = {
       'token': mytoken,
       'radius': '34.448333,-111.789167,10',
       'limit': '10',
       }
# 1d) Extract station synoptic data
stationDict = extractmasonet(base_url, args)

# 2) Extract time series data from active sites
# 2a) 'Base' URL
base_url_in = "https://api.synopticdata.com/v2/stations/timeseries"
arg_vars = 'air_temp,precip_accum'
arg_units = 'temp|F,precip|mm'
args_in = {
        'start': start.replace('-', '')+'0000',
        'end': end.replace('-', '')+'0000',
        'obtimezone': 'UTC',
        'vars': arg_vars,
        'stids': '',
        'units': arg_units,
        'token': mytoken}
station_condition_in = 'ACTIVE'
station_name_in = 'QVDA3'

masonet_df = assemble_data_masonet(base_url_in, args_in, stationDict,
                                      data_join=pd.DataFrame(index=flow_df.index),
                                      station_name=station_name_in)

"""## 7. Resample Datasets
* This resamples the precip, temperature, and flow data from USGS, masonet, and vc_crossing
"""

# sumarize flow, precip, air temp on weekly basis
# # resample
# # # flow
flow_df = flow_df.resample("W").mean()
# # # precip and temp masonet
masonet_df['precip_accum_set_1'] = masonet_df['precip_accum_set_1'] \
                                - masonet_df['precip_accum_set_1'].shift(1)
masonet_df['precip_accum_set_1'].where(
            masonet_df['precip_accum_set_1'] > 0, inplace=True)
p_masonet_df = pd.DataFrame(
                            masonet_df['precip_accum_set_1'].
                            resample("W").sum()
                            )

p_masonet_df = p_masonet_df**(1/3)

t_masonet_df = pd.DataFrame(
                            masonet_df['air_temp_set_1'].
                            resample("W").mean()
                            )
# # # precip and temp forecasts
p_vc_forecast_df = pd.DataFrame(
                                vc_forecast_df['Precipitation'].
                                resample("W").sum()
                                )
t_vc_forecast_df = pd.DataFrame(
                                vc_forecast_df['Temperature'].
                                resample("W").mean()
                                )

"""## 8. Cross Plots Before Normalization
* This shows how well correlated the temperature, precipitation, and flow data are from Masonet and USGS, respectively. Before 'normalization' (scaling from 0 to 1). This figure shows that the relationships between flow, precipitation, and tempreature are difficult to discern because of the non-normal distribution of flow and precipitation (heavily skewed right)
"""

t = pd.concat([flow_df, p_masonet_df, t_masonet_df], axis=1)
scatter_matrix(t, alpha=0.5, figsize=(15,15))
plt.show()
del(t)

"""## 9. Subset, normalize, and lag datasets
* This will lag the datasets by one week (for auto-regression), as well as normalize the data to values ~ between -1 and 1 and using the robust scaler
"""

# # subset, normalize, and lag
flow_df, scale1 = norm_it(start, end, flow_df, 'flow', l_back=1,
                             cscaler=MinMaxScaler())
p_masonet_df, scale2 = norm_it(start, end, p_masonet_df,
                                  'precip_accum_set_1', l_back=1,
                                  cscaler=MinMaxScaler())
t_masonet_df, scale3 = norm_it(start, end, t_masonet_df,
                                  'air_temp_set_1', l_back=1,
                                  cscaler=MinMaxScaler())

# # take greater number of look-backs if window > 1
if window > 1: 
  for win in range(2,window+1):
    flow_df = norm_it(start, end, flow_df, 'flow_norm', l_back=win, toscale=False)
    p_masonet_df = norm_it(start, end, p_masonet_df, 'precip_accum_set_1_norm', l_back=win, toscale=False)
    t_masonet_df = norm_it(start, end, t_masonet_df, 'air_temp_set_1_norm', l_back=win, toscale=False)

"""## 10. Cross Plots After Normalization
* This shows how well correlated the temperature, precipitation, and flow data are from Masonet and USGS, respectively. After 'normalization' (scaling ~ 1 to 1) and transformation (to ~ gaussian). Additionally, includes lagged comparison (-1 week). We see that the relationships between flow and temperature are much more concrete, although precipitation leaves something to be desired...
"""

t = pd.concat([flow_df['flow_norm'], flow_df['flow_norm1'], p_masonet_df['precip_accum_set_1_norm'], p_masonet_df['precip_accum_set_1_norm1'], 
t_masonet_df['air_temp_set_1_norm'], t_masonet_df['air_temp_set_1_norm1']], axis=1)
scatter_matrix(t, alpha=0.5, figsize=(15,15))
plt.show()
del(t)

"""# Regression
* For Homework

## 1. Build and Export an Auto-Regressive Model
* This part of the code creates an auto-regressive model using the raw temperature, flow, and precip data to create a prediction for our two week and sixteen week forecasts for flow
"""

# Step 1: pick regression variables
# Step 2: pick periods of regression (train)
# Step 3: subset data to regression (trains)
t = pd.concat([flow_df, p_masonet_df, t_masonet_df], axis=1)
t = clean_dataset(t)
t = t.reset_index()

# Step 4: Fit a linear regression to 'train' data using sklearn
# for (i) 1 and 2 week prediction

# # predictive variables (all normalized between 0 and 1) =
# # # 1) 'flow_norm_tm1' (log of flow last week)
# # # 2) 'air_temp_set_1_norm' (mean weekly temperature, over 10 years)
# # # 2) 'precip_accum_set_1_norm' (sum weekly preciptation, over 10 years)
x = t[['flow_norm1',
       'air_temp_set_1_norm',
       'precip_accum_set_1_norm']]
# # dependent variable = 'flow' (log of flow this week)
y = t['flow_norm']
# # use predifined function (makemodel) to generate model
m, s = makemodel(x, y)

# Step 5: Make a prediction for (i) 1 and 2 week,

# (i) week prediction
# set lastweekflow data as most recent week of data
x_lastweekflow = t['flow_norm'].iloc[-1]
# # set last week precip variation and temperature
# x_lastweektemp = t['air_temp_set_1_norm'].iloc[-1]
# x_lastweekprecip = t['precip_accum_set_1_norm'].iloc[-1]

print("AR Week 1 and 2 forecast prediction:")

for i in range(2):
    # set last week precip variation and temperature
    x_lastweekprecip = scale2.transform(p_vc_forecast_df['Precipitation'].
                                        iloc[i].reshape(1, -1))
    x_lastweektemp = scale3.transform(t_vc_forecast_df['Temperature'].
                                      iloc[i].reshape(1, -1))
    # set the week i name
    name = "Week {0}:".format(i+1)
    # predict week i flow, using flow, temperature, and precip
    x_lastweekflow = m.intercept_ + m.coef_[0] * x_lastweekflow \
                                  + m.coef_[1] * x_lastweektemp \
                                  + m.coef_[2] * x_lastweekprecip
    # print week, flow (forecast), precip and temp (predictive variables)
    print(name, "Flow (cfs) =",
                np.round(np.exp(denorm_it(x_lastweekflow, scale1)), 2),
                "Precip (mm)=",
                np.round(denorm_it(x_lastweekprecip, scale2)**3, 2),
                "Temp (deg F)=",
                np.round(denorm_it(x_lastweektemp, scale3), 2))


print("\n")

# Step 4: Fit a linear regression to 'train' data using sklearn
nmlist = []
for win in range(1,window+1):
  # print('flow_norm'+str(win))
  nmlist.append('flow_norm'+str(win))

# for (ii) semester forecast
x = t[nmlist]
# # dependent variable = 'flow' (log of flow this week)
y = t['flow_norm']
# # use predifined function (makemodel) to generate model
m2, s2 = makemodel(x, y)

# Step 5: Make a prediction for (ii) semester forecast

# (ii) semester forecast
# set lastweek as first week of semester
x_lastweekindex = t['flow_norm'][
                            (t['datetime'] >= '2020-08-20') &
                            (t['datetime'] < '2020-08-27')
                            ].index.values

x_lastweekflow = t['flow_norm'].iloc[x_lastweekindex].values
x_prevweeksflow = t[nmlist].iloc[x_lastweekindex].values[0]

print("AR Semester forecast prediction:")

# make a list of coefficients
coeflist = []
for win in range(0,window):
  coeflist.append(m2.coef_[win])
coeflist = np.array(coeflist)

# cycle through all weeks
for i in range(16): # 16
    name = "Week {0}:".format(i+1)
    coefsum = x_prevweeksflow*coeflist
    # predict week i flow, using flow, temperature, and precip
    x_lastweekflow = m2.intercept_ + coefsum.sum()
    # print week, flow (forecast)
    print(name, "Flow (cfs) =",
                np.round(np.exp(denorm_it(x_lastweekflow, scale1)), 2))
    # shuffle values in prevweekflow array
    x_prevweeksflow = np.insert(x_prevweeksflow[:-1],0,x_lastweekflow)

"""## 2. Score the Auto-Regression and Compare to Reality
* This portion reflects the score and fit of the autoregression when compared to reality

* This first part is for the seasonal forecast
"""

# two week forecast 
print('The R-sqaured of the two week forecast was: ', round(s,2))
# prediction using seasonal forecast
q_pred1 = m.predict(t[['flow_norm1',
                      'air_temp_set_1_norm',
                      'precip_accum_set_1_norm']])

# make plot
fig, ax = plt.subplots()
ax.plot(t['datetime'].to_frame(), t['flow_norm'], label='true flow, normalized')
ax.plot(t['datetime'].to_frame(), q_pred1, 'r:', label='training flow, normalized')
ax.set(title="Observed v training flow, two week model", xlabel="Date", 
        ylabel="Weekly Avg Flow, normalized")
ax.legend()
fig.show()

"""* This second part is for the weekly forecast"""

# seasonal forecasts
print('The R-sqaured of the seasonal forecast was: ', round(s2,2))

# prediction using two week model forecast
q_pred2 = m2.predict(t[nmlist].to_numpy())

# make plot
fig, ax = plt.subplots()
ax.plot(t['datetime'].to_frame(), t['flow_norm'], label='true flow, normalized')
ax.plot(t['datetime'].to_frame(), q_pred2, 'r:', label='training flow, normalized')
ax.set(title="Observed v training flow, seasonal model", xlabel="Date", 
        ylabel="Weekly Avg Flow, normalized")
ax.legend()
fig.show()

"""# LSTM 2, With Streamflow Data
* This is a variation of a method used to calculate depth to water

## 1. Functions
* Including the RNN network
"""

def rmse(y1, y2):
    return np.sqrt(mean_squared_error(y1, y2))

def sliding_windows(data, seq_length):
    """This takes in a dataset and sequence length, and uses this
    to dynamically create (multiple) sliding windows of data to fit

    This is worth investigating to better understand
    """
    x = []
    y = []

    for i in range(len(data)-seq_length-1):
        _x = data[i:(i+seq_length)]
        _y = data[i+seq_length]
        x.append(_x)
        y.append(_y)

    return np.array(x),np.array(y)

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, class_size, dropout=0.5, rnn_type='lstm'):
        super(RNN, self).__init__()

        self.input_size = input_size # size of the inputs (i.e. 5 variables) 
        self.hidden_size = hidden_size # number of cells in hidden layer (=40, why?)
        self.class_size = class_size # size of extra hidden layer
        self.num_layers = num_layers # number of hidden layers (=1)
        self.rnn_type = rnn_type # type of rnn, like lstm

        if self.rnn_type == 'lstm':
            self.rnn = nn.LSTM(
                input_size=self.input_size,       # number of input vars
                hidden_size=self.hidden_size,     # rnn hidden nodes
                num_layers=self.num_layers,       # number of rnn layer
                batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
            )
        elif self.rnn_type == 'rnn':
            self.rnn = nn.RNN(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        elif self.rnn_type == 'gru':
            self.rnn = nn.GRU(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        else:
            raise NotImplementedError

        self.dropout = nn.Dropout(dropout) # implementation of dropout, using stated dropout probability
        self.out = nn.Linear(self.hidden_size, self.class_size) # FC layer in our paper. This is the 'extra' layer.

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        if self.rnn_type == 'lstm':
            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
            r_out, _ = self.rnn(x, (h0, c0))
        else:
            r_out, _ = self.rnn(x, h0)

        outs = []    # save all predictions
        for time_step in range(r_out.size(1)):    # calculate output for each time step
            outs.append(self.out(self.dropout((r_out[:, time_step, :]))))
        return torch.stack(outs, dim=1)

"""## 2. Globals"""

# Convert to a DataFrame and render.
# note shape is important because LSTM expects three dimensions
nmlist = ['flow_norm', 'flow_norm1', 'precip_accum_set_1_norm', 'air_temp_set_1_norm']
data = t[nmlist].to_numpy()


# fraction of data to include in training set (1=all,0=none, remainder in test set)
test_frac = 0.67

# For LSTM
num_epochs = 4000 # number of times iterating through the model
learning_rate = 0.01 # rate of learning


# Define rnn model
# self.input_size = input_size # size of the inputs (i.e. 5 variables)
ins = 3
# self.hidden_size = hidden_size # number of cells in hidden layer
his = 40
# self.num_layers = num_layers # number of hidden layers (=1)
nlay = 1
# self.class_size = class_size # size of the extra hidden linear layer (I think this is number of layers...
cs = 1
# drop_out rate (to prevent over fitting)
do = 0.5
# type of neural network (lstm)
rt = 'lstm'


# Define rnn model
model = RNN(input_size=ins, hidden_size=his, num_layers=nlay, class_size=cs, dropout=do, rnn_type=rt)
# Define optimization function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   # optimize all rnn parameters
# Define loss function
loss_func = nn.MSELoss()

"""## 3. Code"""

# define predictors and predictands, and expand dimensions (adding a third 1-D value)
x = np.expand_dims(data[:,1:], axis=0)
y = np.expand_dims(data[:,0], axis=(0,2))

# divide into test and train size, essentially into thirds
train_size = int(y.shape[1] * test_frac)
test_size = y.shape[1] - train_size

# convert to torch tensors
dataX = Variable(torch.Tensor(np.array(x)))
dataY = Variable(torch.Tensor(np.array(y)))

# create train vars
trainX = Variable(torch.Tensor(np.array(x[:,0:train_size,:])))
trainY = Variable(torch.Tensor(np.array(y[:,0:train_size,:])))

# create test vars
testX = Variable(torch.Tensor(np.array(x[:,train_size:,:])))
testY = Variable(torch.Tensor(np.array(y[:,train_size:,:])))

epoch_array = []
loss_array = []

# Start training
for iter in range(num_epochs+1):
    # what about batch sizes>?
    
    model.train() # QH Intialize Training
    optimizer.zero_grad()  # clear gradients for this training step
    prediction = model(trainX) # Input in variables
    loss = loss_func(prediction, trainY) # calculate loss
    loss.backward()        # back propagation, compute gradients
    optimizer.step()       # move step forward
    if iter % 100 == 0:    # iterate
        print("iteration: %s, loss: %s" % (iter, loss.item()))
        epoch_array.append(iter)
        loss_array.append(loss.item())

print('')
print('the relationship between loss and epochs for given hyper parameters')
plt.plot(epoch_array, loss_array)
plt.show()

# Start evaluating model
model.eval()
y_pred_dep = model(testX).detach().numpy() # use test data

"""# LSTM Method 1, with Streamflow Data
From GitHub: https://colab.research.google.com/drive/1XmOoVu8Tt4iSCTrGXYBpbyD8Tq8RSvoh?authuser=1#scrollTo=vIWvJCpOVmwU

## 1. Functions
* For LSTM 1, including sliding windows and network structure
"""

def sliding_windows(data, seq_length):
    """This takes in a dataset and sequence length, and uses this
    to dynamically create (multiple) sliding windows of data to fit

    This is worth investigating to better understand
    """
    x = []
    y = []

    for i in range(len(data)-seq_length-1):
        _x = data[i:(i+seq_length)]
        _y = data[i+seq_length]
        x.append(_x)
        y.append(_y)

    return np.array(x),np.array(y)

"""### Creating LSTM Model
* Define a class LSTM from nn-Module
* Further reading: 
https://stackabuse.com/introduction-to-pytorch-for-classification/ 
"""

class LSTM(nn.Module):
    # input_size = the number of features in the input

    # num_classes = dimensions of the export layer

    # hidden_size = dimensions of the hidden layer

    # num_layers = number of layers to use
    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        # Super
        super(LSTM, self).__init__()
        
        # attributes
        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.seq_length = seq_length

        # the LSTM takes inputs and exports hidden states
        # note we are only passing in dimensions here
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        
        # the LSTM takes hidden states space to tag space
        self.fc = nn.Linear(hidden_size, num_classes)

      # this moves weights forward through network
      # I'm not sure how this works!!
    def forward(self, x):
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
        
        h_out = h_out.view(-1, self.hidden_size)
        
        out = self.fc(h_out)
        
        return out

"""##2. Globals
* For LSTM
"""

# data source
# note shape is important because LSTM expects three dimensions
data = t['flow_norm'].to_numpy().reshape(-1, 1)

# sliding sequence length (by default, the same as windows set in globals for previous step)
seq_length = 4 # window

# fraction of data to include in training set (1=all,0=none, remainder in test set)
test_frac = 0.67

# For LSTM
num_epochs = 2000 # number of times iterating through the model
learning_rate = 0.01 # rate of learning

input_size = 1 # nodes on the input (should be 1)
hidden_size = 10 # number of nodes in hidden layer 
num_layers = 1 # number of hidden layers

num_classes = 1 # nodes in output (should be 1)

"""## 3. Import Data
* This is the first step of the PyTorch procedure
* Sets training data equal to normalized flow data
* Uses the function sliding_windows to define the set of training and test data
* Sets variables as Torch Tensors
"""

# define sliding windows (note, these are defined in the globals)
x, y = sliding_windows(data, seq_length)

# divide into test and train size, essentially into thirds
train_size = int(len(y) * test_frac)
test_size = len(y) - train_size

# convert to torch tensors
dataX = Variable(torch.Tensor(np.array(x)))
dataY = Variable(torch.Tensor(np.array(y)))

# create train vars
trainX = Variable(torch.Tensor(np.array(x[0:train_size])))
trainY = Variable(torch.Tensor(np.array(y[0:train_size])))

# create test vars
testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))
testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))

"""## 4. Train model
* Note the global choices made below
"""

# -------
# create model and define criterion and optimizer
# -------

lstm = LSTM(num_classes, input_size, hidden_size, num_layers)

criterion = torch.nn.MSELoss()    # mean-squared error for regression
optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # Adam as optimizer

epoch_array = []
loss_array = []

# Train the model
for epoch in range(num_epochs):
    # note no mini-batching within the loop. This is done using the sliding window approach

    # zero gradients
    optimizer.zero_grad()

    # make predictions
    outputs = lstm(trainX)
    
    # obtain the loss function
    loss = criterion(outputs, trainY)

    # back propogation
    loss.backward()
    
    # update the weights backward
    optimizer.step()

    # print results of fitting if epochal condition is met
    if epoch % 100 == 0:
      print("Epoch: %d, loss: %1.5f" % (epoch, loss.item()))
      epoch_array.append(epoch)
      loss_array.append(loss.item())

print('')
print('the relationship between loss and epochs for given hyper parameters')
plt.plot(epoch_array, loss_array)
plt.show()

"""## 5. Score the LSTM and Compare to Reality
* This portion reflects the score and fit of the autoregression when compared to reality 
"""

# explicitly tell the model we are evaluating it now
lstm.eval()

# make a prediction using the model on all of the data
data_predict = lstm(dataX)

# convert to numpy array
data_predict = data_predict.data.numpy()
dataY_plot = dataY.data.numpy()

# score the LSTM
print('The R-squared value for LSTM is ', r2_score(dataY_plot, data_predict))

# plot
fig, ax = plt.subplots()
ax.plot(t['datetime'].to_frame().iloc[seq_length:-1], dataY_plot, label='true flow, normalized')
ax.plot(t['datetime'].to_frame().iloc[seq_length:-1], data_predict, 'r:', label='training flow, normalized')
ax.set(title="Observed v training flow, weekly model", xlabel="Date", 
        ylabel="Weekly Avg Flow, normalized")
ax.legend()
# fig.set_axvline(x=train_size, c='r', linestyle='--')
fig.show()

"""# Compare predicted values from methods
* How different were the LSTM predictions from the autoregressive method?
"""

# comparing predictions and predictands, test data, lstm 1
test_predict = lstm(testX)
test_predict_plot = test_predict.data.numpy()
testY_plot = testY.data.numpy()
print("the r^2 between predictions and predictands, test data, lstm 1 ", r2_score(testY_plot,test_predict_plot))
plt.scatter(testY_plot,test_predict_plot)
plt.show()

# look at fit between predictors and predictands, lstm 2
print('Blue is the fit between the training predicted outputs and reality for LSTM 2 (blue)')
plt.scatter(prediction[0,:,0].detach().numpy(),trainY[0,:,0].detach().numpy())


# look at fit between predictors and predictands
print('Orange is the fit between the testing predicted outputs and reality for LSTM (orange)')
plt.scatter(y_pred_dep[0,:,0],testY[0,:,0].detach().numpy())

# using only the streamflow in autoregerssion
print("the r^2 between autoregression (streamflow only) and lstm r2_score ", r2_score(q_pred2[seq_length:-1],data_predict))
plt.scatter(q_pred2[seq_length:-1],data_predict)
plt.xlabel('autoregression (streamflow only)')
plt.ylabel('lstm (streamflow only)')
plt.show()

# using the multiple variable autoregression
print("the r^2 between autoregression (multivariable) and lstm r2_score ", r2_score(q_pred1[seq_length:-1],data_predict))
plt.scatter(q_pred2[seq_length:-1],data_predict)
plt.xlabel('autoregression (multivariable)')
plt.ylabel('lstm (streamflow only)')
plt.show()

"""# LSTM Method 2 - Raw
* https://github.com/jfzhang95/LSTM-water-table-depth-prediction/
* This is a variation of a method used to calculate depth to water

## 1. Functions
* Including the RNN network
"""

def rmse(y1, y2):
    return np.sqrt(mean_squared_error(y1, y2))

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, class_size, dropout=0.5, rnn_type='lstm'):
        super(RNN, self).__init__()

        self.input_size = input_size # size of the inputs (i.e. 5 variables) 
        self.hidden_size = hidden_size # number of cells in hidden layer (=40, why?)
        self.class_size = class_size # size of extra hidden layer
        self.num_layers = num_layers # number of hidden layers (=1)
        self.rnn_type = rnn_type # type of rnn, like lstm

        if self.rnn_type == 'lstm':
            self.rnn = nn.LSTM(
                input_size=self.input_size,       # number of input vars
                hidden_size=self.hidden_size,     # rnn hidden nodes
                num_layers=self.num_layers,       # number of rnn layer
                batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
            )
        elif self.rnn_type == 'rnn':
            self.rnn = nn.RNN(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        elif self.rnn_type == 'gru':
            self.rnn = nn.GRU(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        else:
            raise NotImplementedError

        self.dropout = nn.Dropout(dropout) # implementation of dropout, using stated dropout probability
        self.out = nn.Linear(self.hidden_size, self.class_size) # FC layer in our paper. This is the 'extra' layer.

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        if self.rnn_type == 'lstm':
            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
            r_out, _ = self.rnn(x, (h0, c0))
        else:
            r_out, _ = self.rnn(x, h0)

        outs = []    # save all predictions
        for time_step in range(r_out.size(1)):    # calculate output for each time step
            outs.append(self.out(self.dropout((r_out[:, time_step, :]))))
        return torch.stack(outs, dim=1)

"""## 2. Globals"""

# This is for google sheets imports
gc = gspread.authorize(GoogleCredentials.get_application_default())

"""## 3. Code"""

# Set scalers
ss_X_dep = StandardScaler()
ss_y_dep = StandardScaler()

# Noted that the demo data are processed manually, so they are not real data,
# but they still can reflect the correlation between the original data.
worksheet = gc.open('demo_for_Hull_HW13').sheet1

# get_all_values gives a list of rows.
rows = worksheet.get_all_values()
print(rows)

# Convert to a DataFrame and render.
data = pd.DataFrame.from_records(rows[1:],columns=rows[0])

# clean dataset
Inputs = data.drop('Year', axis=1).drop('Depth', axis=1)
Outputs = data['Depth']

# set as numpy array
Inputs = Inputs.values # previously as_matrix(), depricated
Outputs = Outputs.values.reshape(-1, 1) # previously as_matrix().reshape(-1,1), depricated

# split into train and test 
# First 12 years of data
X_train_dep = Inputs[0:144] # inputs
y_train_dep = Outputs[0:144] # outputs

# Last 2 years of data
X_test_dep = Inputs[144:] # inputs

print("X_train_dep shape", X_train_dep.shape) #inputs training
print("y_train_dep shape", y_train_dep.shape) #outputs training
print("X_test_dep shape", X_test_dep.shape) #inputs test

# bring back together training and test inputs
X = np.concatenate([X_train_dep, X_test_dep], axis=0)


# Standardization
X = ss_X_dep.fit_transform(X)

# Split standardized data again into test and train portions
X_train_dep_std = X[0:144]
# trainsform the dependent data
y_train_dep_std = ss_y_dep.fit_transform(y_train_dep)

# All 14 years of data
X_test_dep_std  = X # I think this may be an error too, because we shouldn't
# have all data (168 rows) here, only the last 168-144 = 24 rows
X_train_dep_std = np.expand_dims(X_train_dep_std, axis=0) # expand dimensions adds a third, 1-D value for alter
y_train_dep_std = np.expand_dims(y_train_dep_std, axis=0) # expand dimensions adds a third, 1-D value for later
X_test_dep_std = np.expand_dims(X_test_dep_std, axis=0) # expand dimensions adds a third, 1-D value for later

# Transfer to Pytorch Variable
X_train_dep_std = Variable(torch.from_numpy(X_train_dep_std).float())
y_train_dep_std = Variable(torch.from_numpy(y_train_dep_std).float())
X_test_dep_std = Variable(torch.from_numpy(X_test_dep_std).float())

# Define rnn model
# You can also choose rnn_type as 'rnn' or 'gru'
    # self.input_size = input_size # size of the inputs (i.e. 5 variables) 
    # self.hidden_size = hidden_size # number of cells in hidden layer (=40, why?)
    # self.class_size = class_size # size of the extra hidden layer
    # self.num_layers = num_layers # number of hidden layers (=1)
    # self.rnn_type = rnn_type # type of rnn, like lstm
model = RNN(input_size=5, hidden_size=40, num_layers=1, class_size=1, dropout=0.5, rnn_type='lstm')
# Define optimization function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)   # optimize all rnn parameters
# Define loss function
loss_func = nn.MSELoss()

# Start training
for iter in range(20000+1):
    model.train() # QH Intialize Training
    prediction = model(X_train_dep_std) # Input in variables
    loss = loss_func(prediction, y_train_dep_std) # calculate loss
    optimizer.zero_grad()  # clear gradients for this training step
    loss.backward()        # back propagation, compute gradients
    optimizer.step()       # move step forward
    if iter % 100 == 0:    # iterate
        print("iteration: %s, loss: %s" % (iter, loss.item()))

# Start evaluating model
model.eval()

# This is a bit tricky, because it includes the data used in training in 
# testing the fit of the model. I think very wrong (but maybe not?)! 
y_pred_dep_ = model(X_test_dep_std).detach().numpy() # use test data
y_pred_dep = ss_y_dep.inverse_transform(y_pred_dep_[0, 144:]) # de-scale data

# Use 
print('the value of R-squared of Evaporation is ', r2_score(Outputs[144:], y_pred_dep))
print('the value of Root mean squared error of Evaporation is ', rmse(Outputs[144:], y_pred_dep))

#plot
f, ax1 = plt.subplots(1, 1, sharex=True, figsize=(6, 4))

ax1.plot(Outputs[144:]) # , color="blue", linestyle="-", linewidth=1.5, label="Measurements")
ax1.plot(y_pred_dep, color="green", linestyle="--", linewidth=1.5, label="Proposed model")

plt.legend(loc='upper right')
plt.xticks(fontsize=8,fontweight='normal')
plt.yticks(fontsize=8,fontweight='normal')
plt.xlabel('Time (Month)', fontsize=10)
plt.ylabel('Water table depth (m)', fontsize=10)
plt.xlim(0, 25)
plt.savefig('results.png', format='png')
plt.show()