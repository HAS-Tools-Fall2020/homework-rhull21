# -*- coding: utf-8 -*-
"""Modified From: Copy of Hull_HW13_ML_Mod.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pTFLFd4W2P5ARacIv07drXrG-9In1Rd

----------------------

# Introduction

The function is to assemble hydrologic and meterological data to create streamflow forecasts for streamflow at Verde River in central Arizona. See the outline below for a detailed description of the code utilized by this notebook. In general we are comparing traditional ML (ie linear autocorrelation) techniques with 'novel' DL (i.e. LSTM) techniques

# Outline of 'how' I generate the forecast

## Data Assembly and Prep
1. Define modules
2. Define functions
3. Define globals
4. Import forecast 'T' and 'P' - Virtual Crossing
5. Import and assemble historical flow - USGS
6. Import and assemble historical 'T' and 'P' - Mesonet
7. Resample Datasets
8. Cross Plots Before Normalization
9. Subset, normalize, and lag datasets
10. Cross Plots After Normalization

## Regression
1. Build and Export an Auto-Regressive Model
2. Show fit between real and training data

## PyTorch LSTM (2)
1. Functions
2. Globals
3. Code

## Compare Predicted Values From Different Methods
"""

# %%
## 1. Define Modules

# generic
import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
from pandas.plotting import scatter_matrix

# functions
import json
import urllib.request as req
import urllib
from sklearn.linear_model import LinearRegression

# for scaling
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer

# for scoring
from sklearn.metrics import r2_score, mean_squared_error

# for DL
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error


# %%
""" 2. Define Functions
"""

def clean_dataset(df):
    """Removes all infinity, nan, and numbers out of range
    from: https://stackoverflow.com/questions/31323499/
    sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for
    """
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)


def makemodel(x, y):
    """returns a multiple regression model

       using sklearn linear regression tool

       returns the model object = model
       returns the score = score

       takes 2 required variables, x and y
       x is the predictive (independent) variable(s)
       y is the predicted (dependent) variable

       both x and y need to be pandas dataframes, where x
       contains ALL of the predictive (independent) variables
       to be used.

       IMPORTANT: 'x' needs to be a list of column titles, even
       if only one predictive variable is passed

       if dimensions[x] = 1, then single predictive variable
       if dimensions[x] > 1, then multiple predictive variables

       example:
       x = train_week[['flow_tm1']] # use double brackets here
       y = train_week['flow'] # use single brackets here
       m, x = makemodel(x,y)
       """

    model = LinearRegression()
    y = y.values
    if x.shape[1] == 1:
        x = x.values.reshape(-1, 1)
    model.fit(x, y)
    score = model.score(x, y)
    return model, score


def extractmasonet(base_url, args):
    """takes
       1) a string of 'base_url'
       2) a dictionary of api arguments
       3) a string of 'token'
       specific to the metamot API

       See more about metamot:
       https://developers.synopticdata.com/mesonet/explorer/
       https://developers.synopticdata.com/about/station-variables/
       https://developers.synopticdata.com/mesonet/v2/getting-started/

       returns a dictionary
       containing the response of a JSON 'query'
       """

    # concat api arguments (careful with commas)
    apiString = urllib.parse.urlencode(args)
    apiString = apiString.replace('%2C', ',')

    # concat the API string to the base_url to create full_URL
    fullUrl = base_url + '?' + apiString
    print('full url =', fullUrl, '\n')

    # process data (use url to query data)
    # return as dictionary
    response = req.urlopen(fullUrl)
    responseDict = json.loads(response.read())

    return responseDict


def assemble_data_masonet(base_url, args, stationDict,
                          data_join,
                          station_condition='ACTIVE',
                          station_name=False):
    """takes the basics for a data extraction
        and pulls out only the stations that meet a certain condition

        base_url = url at masonet for extraction
        args = arguments, including token and parameters
        station_Dict = a previously created response Dictionary
            of stations
        data_join = an external pandas dataset to join the data to.
            Must contain a datetime index
        station_condition = the condition used to crete response for data
            default is to look for only active stations. Only acceptable
            values are 'ACTIVE' and 'INACTIVE'
        station_name = by default FALSE. If specified a string, will
            only look for stations by the name specified.

        note by default resamples the data daily on the max

        returns a panda dataframe containint he data wanted
        """

    # 2b) Assemble all relevant dictionaries into a list, based on station name
    stationList = []
    for station in stationDict['STATION']:
        # station name and if is active
        print(station['STID'], station['STATUS'], station["PERIOD_OF_RECORD"],
              "\n")
        # time series data args
        args['stids'] = station['STID']
        # extract data from active/inactive stations
        if station['STATUS'] == station_condition:
            # if a station name is specified
            if station_name is not False:
                if (station['STID'] == station_name):
                    # extract data
                    responseDict = extractmasonet(base_url, args)
                    # create a list of all stations
                    stationList.append(responseDict)
            # if station name is not specified
            else:
                # extract data
                responseDict = extractmasonet(base_url, args)
                # create a list of all stations
                stationList.append(responseDict)

    # Checks to see if the API Call returned valid data
    if stationList[0]['SUMMARY']['RESPONSE_CODE'] == -1:
        print(stationList[0]['SUMMARY']['RESPONSE_MESSAGE'])
        return "nothing"

    # 2d) convert all data pd
    # list of keys under observations (for use in inner loop)
    for station in stationList:
        # if station id has the station_name, or station_name is False
        if (station['STATION'][0]['STID'] == station_name) or \
                            (station_name is False):
            print(station['STATION'][0]['STID'])
            for key, value in station["STATION"][0]['OBSERVATIONS'].items():
                # creates a list of value related to key
                # temp = station["STATION"][0]['OBSERVATIONS'][key]
                if (key == 'date_time'):
                    # create index
                    df = pd.DataFrame({key: pd.to_datetime(value)})
                else:
                    # concat df
                    df = pd.concat([df, pd.DataFrame({key: value})], axis=1)
            # # set index for df
            df = df.set_index('date_time')
            # resample on day
            df = df.resample('D').max()
            # join df to data dataframe
            data_join = data_join.join(df,
                                       rsuffix="_"+station['STATION'][0]['STID'
                                                                         ])
            df = pd.DataFrame()
    return data_join


def Kelvin_2_Faren(K_temp):
    """takes a temperature in Kelvin

    returns a temperature in Fareignheit
        """
    return (K_temp - 273.15)*(9/5) + 32


def norm_it(startdate, enddate, dfin, dfname, l_back=1, toscale=True,
            cscaler=MinMaxScaler(feature_range=(0, 1))):
    """
    This function noramlizes a column of data from a pandas dataframe
    using the predefined scaler feature of sklearn
    to create a ~ normal distribution
    and allow for better fitting between different types of variables
    in a multivariable regression

    It also lags the data by a specified number of weeks (look_back)

    Takes:
    A start and end date (strings)
        startdate =
        enddate =
    A dataframe (pandas)
        dfin =
    The name of a single column of data to normalize (string)
        dfname =
    A specified number of look backs (integer)
        l_back = [1]
    A Boolean to decide if to scale the data (i.e. if not desired or already done)
        toscale = [True]
    An optional scaler
        cscaler = [MinMaxScaler(feature_range=(0, 1))]

    Returns:
    The dataframe with a column of normalized, and lagged, data
    The scaler model that can be used to 'inverse' transform
        """

    # # subset
    dfin = dfin.loc[startdate:enddate]
    if toscale is True:
        # # normalize
        scaler = cscaler
        # # add normalized to dataset
        dfin[dfname+'_norm'] = scaler.fit_transform(dfin[
                                                dfname].to_numpy().reshape(-1, 1))
        # # lag
        dfin[dfname+'_norm'+str(l_back)] = dfin[dfname+'_norm'].shift(l_back)
        return dfin, scaler
    else:
        dfin[dfname+str(l_back)] = dfin[dfname].shift(l_back)
        return dfin



def denorm_it(val, scaler):
    """De normalizes a single value

    Takes:
    A scaled value (a single number)
        val =
    A scaler from sklearn
        scaler =
    """
    # # inverse transform a single value
    newval = scaler.inverse_transform(val.reshape(1, -1))
    return newval

# %%
"""3. Define Globals
* Including the max and min dates used to extract data via API from data source, as well as the date ranges used to run regression
"""

# General
start = '2010-02-01'
end = '2020-12-31'
window = 1

# %%
"""## 4. Import forecast 'T' and 'P' - Virtual Crossing
* Note that a limited number of 'free' API queries are allowed per day (~1000)
* It may be necessary for user to change 'vskey' by creating a new virtual crossing username and locating account key at this website: https://www.visualcrossing.com/weather/weather-data-services#/accountdetails
"""

# keys for accessing forecast
vskey = 'SL1F2XI46LCJAUMYAGG6GMV5B'
vslat = str(34.448333)
vslong = str(-111.789167)
vsurl = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?locations='+vslat+','+vslong+'&aggregateHours=24&unitGroup=us&shortColumnNames=false&contentType=csv&key='+vskey

# access forecast
vc_forecast_df = pd.read_csv(vsurl,
                             parse_dates=['Date time'],
                             index_col='Date time'
                             )

# %%
"""## 5. Import and assemble historical flow - USGS"""

# adjust path as necessary
site = '09506000'
url = "https://waterdata.usgs.gov/nwis/dv?cb_00060=on" \
      "&format=rdb&site_no="+site+"&referred_module=sw&" \
      "period=&begin_date="+start+"&end_date="+end

# read in data
flow_df = pd.read_table(url, sep='\t', skiprows=30,
                        names=['agency_cd', 'site_no',
                               'datetime', 'flow', 'code'],
                        parse_dates=['datetime'],
                        index_col='datetime'
                        )

# re-instantiate data with just the flow values
flow_df = np.log(flow_df[['flow']])
# flow_df = flow_df[['flow']]
flow_df.index = flow_df.index.tz_localize(tz="UTC")

# %%
"""## 6. Import and assemble historical 'T' and 'P' - Mesonet"""

# 1) quickly look for nearby stations
# 1a) Token
# # IMPORTNAT: I overused my token ('a836998da79e4faeac2bf7f5cda57a6e')
# # so I am only able to use the demo token below
mytoken = 'demotoken'
# 1b) 'Base' URL
base_url = "https://api.synopticdata.com/v2/stations/metadata"
# 1c) nearby stations
# look for 10 nearest stations within 10 miles of usgs gaging station
args = {
       'token': mytoken,
       'radius': '34.448333,-111.789167,10',
       'limit': '10',
       }
# 1d) Extract station synoptic data
stationDict = extractmasonet(base_url, args)

# 2) Extract time series data from active sites
# 2a) 'Base' URL
base_url_in = "https://api.synopticdata.com/v2/stations/timeseries"
arg_vars = 'air_temp,precip_accum'
arg_units = 'temp|F,precip|mm'
args_in = {
        'start': start.replace('-', '')+'0000',
        'end': end.replace('-', '')+'0000',
        'obtimezone': 'UTC',
        'vars': arg_vars,
        'stids': '',
        'units': arg_units,
        'token': mytoken}
station_condition_in = 'ACTIVE'
station_name_in = 'QVDA3'

masonet_df = assemble_data_masonet(base_url_in, args_in, stationDict,
                                      data_join=pd.DataFrame(index=flow_df.index),
                                      station_name=station_name_in)


# %%
"""## 7. Resample Datasets
* This resamples the precip, temperature, and flow data from USGS, masonet, and vc_crossing
"""

# sumarize flow, precip, air temp on weekly basis
# # resample
# # # flow
flow_df = flow_df.resample("W").mean()
# # # precip and temp masonet
masonet_df['precip_accum_set_1'] = masonet_df['precip_accum_set_1'] \
                                - masonet_df['precip_accum_set_1'].shift(1)
masonet_df['precip_accum_set_1'].where(
            masonet_df['precip_accum_set_1'] > 0, inplace=True)
p_masonet_df = pd.DataFrame(
                            masonet_df['precip_accum_set_1'].
                            resample("W").sum()
                            )

p_masonet_df = p_masonet_df**(1/3)

t_masonet_df = pd.DataFrame(
                            masonet_df['air_temp_set_1'].
                            resample("W").mean()
                            )
# # # precip and temp forecasts
p_vc_forecast_df = pd.DataFrame(
                                vc_forecast_df['Precipitation'].
                                resample("W").sum()
                                )
t_vc_forecast_df = pd.DataFrame(
                                vc_forecast_df['Temperature'].
                                resample("W").mean()
                                )


# %%
"""## 8. Cross Plots Before Normalization
* This shows how well correlated the temperature, precipitation, and flow data are from Masonet and USGS, respectively. Before 'normalization' (scaling from 0 to 1). This figure shows that the relationships between flow, precipitation, and tempreature are difficult to discern because of the non-normal distribution of flow and precipitation (heavily skewed right)
"""

t = pd.concat([flow_df, p_masonet_df, t_masonet_df], axis=1)
scatter_matrix(t, alpha=0.5, figsize=(15,15))
plt.show()
del(t)

"""## 9. Subset, normalize, and lag datasets
* This will lag the datasets by one week (for auto-regression), as well as normalize the data to values ~ between -1 and 1 and using the robust scaler
"""

# # subset, normalize, and lag
flow_df, scale1 = norm_it(start, end, flow_df, 'flow', l_back=1,
                             cscaler=MinMaxScaler())
p_masonet_df, scale2 = norm_it(start, end, p_masonet_df,
                                  'precip_accum_set_1', l_back=1,
                                  cscaler=MinMaxScaler())
t_masonet_df, scale3 = norm_it(start, end, t_masonet_df,
                                  'air_temp_set_1', l_back=1,
                                  cscaler=MinMaxScaler())

# # take greater number of look-backs if window > 1
if window > 1: 
  for win in range(2,window+1):
    flow_df = norm_it(start, end, flow_df, 'flow_norm', l_back=win, toscale=False)
    p_masonet_df = norm_it(start, end, p_masonet_df, 'precip_accum_set_1_norm', l_back=win, toscale=False)
    t_masonet_df = norm_it(start, end, t_masonet_df, 'air_temp_set_1_norm', l_back=win, toscale=False)

# %%
"""## 10. Cross Plots After Normalization
* This shows how well correlated the temperature, precipitation, and flow data are from Masonet and USGS, respectively. After 'normalization' (scaling ~ 1 to 1) and transformation (to ~ gaussian). Additionally, includes lagged comparison (-1 week). We see that the relationships between flow and temperature are much more concrete, although precipitation leaves something to be desired...
"""

t = pd.concat([flow_df['flow_norm'], flow_df['flow_norm1'], p_masonet_df['precip_accum_set_1_norm'], p_masonet_df['precip_accum_set_1_norm1'], 
t_masonet_df['air_temp_set_1_norm'], t_masonet_df['air_temp_set_1_norm1']], axis=1)
scatter_matrix(t, alpha=0.5, figsize=(15,15))
plt.show()
del(t)

# %%
"""# Regression
* For Homework

## 1. Build and Export an Auto-Regressive Model
* This part of the code creates an auto-regressive model using the raw temperature, flow, and precip data to create a prediction for our two week and sixteen week forecasts for flow
"""

# Step 1: pick regression variables
# Step 2: pick periods of regression (train)
# Step 3: subset data to regression (trains)
t = pd.concat([flow_df, p_masonet_df, t_masonet_df], axis=1)
t = clean_dataset(t)
t = t.reset_index()

# Step 4: Fit a linear regression to 'train' data using sklearn
# for (i) 1 and 2 week prediction

# # predictive variables (all normalized between 0 and 1) =
# # # 1) 'flow_norm_tm1' (log of flow last week)
# # # 2) 'air_temp_set_1_norm' (mean weekly temperature, over 10 years)
# # # 2) 'precip_accum_set_1_norm' (sum weekly preciptation, over 10 years)
x = t[['flow_norm1',
       'air_temp_set_1_norm',
       'precip_accum_set_1_norm']]
# # dependent variable = 'flow' (log of flow this week)
y = t['flow_norm']
# # use predifined function (makemodel) to generate model
m, s = makemodel(x, y)

# Step 5: Make a prediction for (i) 1 and 2 week,

# (i) week prediction
# set lastweekflow data as most recent week of data
x_lastweekflow = t['flow_norm'].iloc[-1]
# # set last week precip variation and temperature
# x_lastweektemp = t['air_temp_set_1_norm'].iloc[-1]
# x_lastweekprecip = t['precip_accum_set_1_norm'].iloc[-1]

print("AR Week 1 and 2 forecast prediction:")

for i in range(2):
    # set last week precip variation and temperature
    x_lastweekprecip = scale2.transform(p_vc_forecast_df['Precipitation'].
                                        iloc[i].reshape(1, -1))
    x_lastweektemp = scale3.transform(t_vc_forecast_df['Temperature'].
                                      iloc[i].reshape(1, -1))
    # set the week i name
    name = "Week {0}:".format(i+1)
    # predict week i flow, using flow, temperature, and precip
    x_lastweekflow = m.intercept_ + m.coef_[0] * x_lastweekflow \
                                  + m.coef_[1] * x_lastweektemp \
                                  + m.coef_[2] * x_lastweekprecip
    # print week, flow (forecast), precip and temp (predictive variables)
    print(name, "Flow (cfs) =",
                np.round(np.exp(denorm_it(x_lastweekflow, scale1)), 2),
                "Precip (mm)=",
                np.round(denorm_it(x_lastweekprecip, scale2)**3, 2),
                "Temp (deg F)=",
                np.round(denorm_it(x_lastweektemp, scale3), 2))


print("\n")

# Step 4: Fit a linear regression to 'train' data using sklearn
nmlist = []
for win in range(1,window+1):
  # print('flow_norm'+str(win))
  nmlist.append('flow_norm'+str(win))

# for (ii) semester forecast
x = t[nmlist]
# # dependent variable = 'flow' (log of flow this week)
y = t['flow_norm']
# # use predifined function (makemodel) to generate model
m2, s2 = makemodel(x, y)

# Step 5: Make a prediction for (ii) semester forecast

# (ii) semester forecast
# set lastweek as first week of semester
x_lastweekindex = t['flow_norm'][
                            (t['datetime'] >= '2020-08-20') &
                            (t['datetime'] < '2020-08-27')
                            ].index.values

x_lastweekflow = t['flow_norm'].iloc[x_lastweekindex].values
x_prevweeksflow = t[nmlist].iloc[x_lastweekindex].values[0]

print("AR Semester forecast prediction:")

# make a list of coefficients
coeflist = []
for win in range(0,window):
  coeflist.append(m2.coef_[win])
coeflist = np.array(coeflist)

# cycle through all weeks
for i in range(16): # 16
    name = "Week {0}:".format(i+1)
    coefsum = x_prevweeksflow*coeflist
    # predict week i flow, using flow, temperature, and precip
    x_lastweekflow = m2.intercept_ + coefsum.sum()
    # print week, flow (forecast)
    print(name, "Flow (cfs) =",
                np.round(np.exp(denorm_it(x_lastweekflow, scale1)), 2))
    # shuffle values in prevweekflow array
    x_prevweeksflow = np.insert(x_prevweeksflow[:-1],0,x_lastweekflow)


# %%
"""## 2. Score the Auto-Regression and Compare to Reality
* This portion reflects the score and fit of the autoregression when compared to reality

* This first part is for the seasonal forecast
"""

# two week forecast 
print('The R-sqaured of the two week forecast was: ', round(s,2))
# prediction using seasonal forecast
q_pred1 = m.predict(t[['flow_norm1',
                      'air_temp_set_1_norm',
                      'precip_accum_set_1_norm']])


"""* This second part is for the weekly forecast"""

# seasonal forecasts
print('The R-sqaured of the seasonal forecast was: ', round(s2,2))
# prediction using two week model forecast
q_pred2 = m2.predict(t[nmlist].to_numpy())


# %%
"""# LSTM 2, With Streamflow Data
* This is a variation of a method used to calculate depth to water

## 1. Functions
* Including the RNN network
"""

def rmse(y1, y2):
    return np.sqrt(mean_squared_error(y1, y2))

def sliding_windows(data, seq_length):
    """This takes in a dataset and sequence length, and uses this
    to dynamically create (multiple) sliding windows of data to fit

    This is worth investigating to better understand
    """
    x = []
    y = []

    for i in range(len(data)-seq_length-1):
        _x = data[i:(i+seq_length)]
        _y = data[i+seq_length]
        x.append(_x)
        y.append(_y)

    return np.array(x),np.array(y)

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, class_size, dropout=0.5, rnn_type='lstm'):
        super(RNN, self).__init__()

        self.input_size = input_size # size of the inputs (i.e. 5 variables) 
        self.hidden_size = hidden_size # number of cells in hidden layer (=40, why?)
        self.class_size = class_size # size of extra hidden layer
        self.num_layers = num_layers # number of hidden layers (=1)
        self.rnn_type = rnn_type # type of rnn, like lstm

        if self.rnn_type == 'lstm':
            self.rnn = nn.LSTM(
                input_size=self.input_size,       # number of input vars
                hidden_size=self.hidden_size,     # rnn hidden nodes
                num_layers=self.num_layers,       # number of rnn layer
                batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
            )
        elif self.rnn_type == 'rnn':
            self.rnn = nn.RNN(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        elif self.rnn_type == 'gru':
            self.rnn = nn.GRU(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                batch_first=True,
            )
        else:
            raise NotImplementedError

        self.dropout = nn.Dropout(dropout) # implementation of dropout, using stated dropout probability
        self.out = nn.Linear(self.hidden_size, self.class_size) # FC layer in our paper. This is the 'extra' layer.

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        if self.rnn_type == 'lstm':
            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
            r_out, _ = self.rnn(x, (h0, c0))
        else:
            r_out, _ = self.rnn(x, h0)

        outs = []    # save all predictions
        for time_step in range(r_out.size(1)):    # calculate output for each time step
            outs.append(self.out(self.dropout((r_out[:, time_step, :]))))
        return torch.stack(outs, dim=1)

# %%
"""## 2. Globals"""

# Convert to a DataFrame and render.
# note shape is important because LSTM expects three dimensions
nmlist = ['flow_norm', 'flow_norm1', 'precip_accum_set_1_norm', 'air_temp_set_1_norm']
data = t[nmlist].to_numpy()


# fraction of data to include in training set (1=all,0=none, remainder in test set)
test_frac = 0.67

# For LSTM
num_epochs = 4000 # number of times iterating through the model
learning_rate = 0.01 # rate of learning


# Define rnn model
# self.input_size = input_size # size of the inputs (i.e. 5 variables)
ins = 3
# self.hidden_size = hidden_size # number of cells in hidden layer
his = 40
# self.num_layers = num_layers # number of hidden layers (=1)
nlay = 1
# self.class_size = class_size # size of the extra hidden linear layer (I think this is number of layers...
cs = 1
# drop_out rate (to prevent over fitting)
do = 0.5
# type of neural network (lstm)
rt = 'lstm'


# Define rnn model
model = RNN(input_size=ins, hidden_size=his, num_layers=nlay, class_size=cs, dropout=do, rnn_type=rt)
# Define optimization function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   # optimize all rnn parameters
# Define loss function
loss_func = nn.MSELoss()


# %%
"""## 3. Code"""

# define predictors and predictands, and expand dimensions (adding a third 1-D value)
x = np.expand_dims(data[:,1:], axis=0)
y = np.expand_dims(data[:,0], axis=(0,2))

# divide into test and train size, essentially into thirds
train_size = int(y.shape[1] * test_frac)
test_size = y.shape[1] - train_size

# convert to torch tensors
dataX = Variable(torch.Tensor(np.array(x)))
dataY = Variable(torch.Tensor(np.array(y)))

# create train vars
trainX = Variable(torch.Tensor(np.array(x[:,0:train_size,:])))
trainY = Variable(torch.Tensor(np.array(y[:,0:train_size,:])))

# create test vars
testX = Variable(torch.Tensor(np.array(x[:,train_size:,:])))
testY = Variable(torch.Tensor(np.array(y[:,train_size:,:])))

epoch_array = []
loss_array = []

# Start training
for iter in range(num_epochs+1):
    # what about batch sizes>?
    
    model.train() # QH Intialize Training
    optimizer.zero_grad()  # clear gradients for this training step
    prediction = model(trainX) # Input in variables
    loss = loss_func(prediction, trainY) # calculate loss
    loss.backward()        # back propagation, compute gradients
    optimizer.step()       # move step forward
    if iter % 100 == 0:    # iterate
        print("iteration: %s, loss: %s" % (iter, loss.item()))
        epoch_array.append(iter)
        loss_array.append(loss.item())

print('')
print('the relationship between loss and epochs for given hyper parameters')
plt.plot(epoch_array, loss_array)
plt.show()

# Start evaluating model
model.eval()
y_pred_dep = model(testX).detach().numpy() # use test data

# %%
"""## 5. Score the LSTM and Compare to Reality
* This portion reflects the score and fit of the lstm when compared to reality and regression
"""

# look at fit between predictors and predictands, lstm training
print('Blue is the fit between the training predicted outputs and reality for LSTM 2 (blue)')
plt.scatter(prediction[0,:,0].detach().numpy(),trainY[0,:,0].detach().numpy())
# score the LSTM
print('The R-squared value for training LSTM is ',
      r2_score(prediction[0,:,0].detach().numpy(), trainY[0,:,0].detach().numpy()))

# look at fit between predictors and predictands, lstm testing
print('Orange is the fit between the testing predicted outputs and reality for LSTM (orange)')
plt.scatter(y_pred_dep[0,:,0],testY[0,:,0].detach().numpy())
# score the LSTM
print('The R-squared value for training LSTM is ',
      r2_score(y_pred_dep[0,:,0], testY[0,:,0].detach().numpy()))
plt.show()

# %%
# Step 5: Make a prediction for (i) 1 and 2 week using the new LSTM model

# (i) week prediction
# set lastweekflow data as most recent week of data
x_lastweekflow = t['flow_norm'].iloc[-1]

print("AR Week 1 and 2 forecast prediction via LSTM:")

for i in range(2):
    # set last week precip variation and temperature
    x_lastweekprecip = scale2.transform(p_vc_forecast_df['Precipitation'].
                                        iloc[i].reshape(1, -1))[0,0]
    x_lastweektemp = scale3.transform(t_vc_forecast_df['Temperature'].
                                      iloc[i].reshape(1, -1))[0,0]

    # set the week i name
    name = "Week {0}:".format(i+1)
    # predict week i flow, using flow, temperature, and precip
    #   in lstm model
    tx = np.expand_dims(np.array([x_lastweekflow, x_lastweekprecip, x_lastweektemp]), axis=(0,1))
    dx = Variable(torch.Tensor(np.array(tx)))
    model.eval()
    x_lastweekflow = model(dx).detach().numpy()[0,0,0]

    # print week, flow (forecast), precip and temp (predictive variables)
    print(name, "Flow (cfs) =",
                np.round(np.exp(denorm_it(x_lastweekflow, scale1)), 2),
                "Precip (mm)=",
                np.round(denorm_it(x_lastweekprecip, scale2)**3, 2),
                "Temp (deg F)=",
                np.round(denorm_it(x_lastweektemp, scale3), 2))


print("\n")
# %%
